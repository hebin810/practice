{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Gradient_descent.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMrVtNCGRZo1D2Xh9qIx1/M",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/spolopsky/ML-practice/blob/main/Gradient_descent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VvFhkBfGREM-",
        "outputId": "c4927ce0-9046-4f3e-e39c-d6480afc1104"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "\r\n",
        "tf.random.set_seed(0) # for reproducibility\r\n",
        "\r\n",
        "X = [1., 2., 3., 4.]\r\n",
        "Y = [1., 3., 5., 7.]\r\n",
        "\r\n",
        "W = tf.Variable(tf.random.normal([1], -100., 100.))\r\n",
        "\r\n",
        "for step in range(300):\r\n",
        "  hypothesis = W * X\r\n",
        "  cost = tf.reduce_mean(tf.square(hypothesis - Y))\r\n",
        "\r\n",
        "  alpha = 0.01\r\n",
        "  gradient = tf.reduce_mean(tf.multiply(tf.multiply(W, X) - Y, X))\r\n",
        "  descent = W - tf.multiply(alpha, gradient)\r\n",
        "  W.assign(descent)\r\n",
        "\r\n",
        "  if step % 10 == 0:\r\n",
        "    print('{:5} | {:10.4f} | {:10.6f}'.format(step, cost.numpy(), W.numpy()[0]))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    0 | 18332.2188 |  47.398293\n",
            "   10 |  3855.3564 |  22.638384\n",
            "   20 |   810.9046 |  11.283927\n",
            "   30 |   170.6631 |   6.076973\n",
            "   40 |    36.0217 |   3.689155\n",
            "   50 |     7.7069 |   2.594144\n",
            "   60 |     1.7524 |   2.091991\n",
            "   70 |     0.5001 |   1.861713\n",
            "   80 |     0.2368 |   1.756112\n",
            "   90 |     0.1814 |   1.707684\n",
            "  100 |     0.1698 |   1.685477\n",
            "  110 |     0.1673 |   1.675292\n",
            "  120 |     0.1668 |   1.670622\n",
            "  130 |     0.1667 |   1.668481\n",
            "  140 |     0.1667 |   1.667498\n",
            "  150 |     0.1667 |   1.667048\n",
            "  160 |     0.1667 |   1.666842\n",
            "  170 |     0.1667 |   1.666747\n",
            "  180 |     0.1667 |   1.666703\n",
            "  190 |     0.1667 |   1.666684\n",
            "  200 |     0.1667 |   1.666674\n",
            "  210 |     0.1667 |   1.666670\n",
            "  220 |     0.1667 |   1.666668\n",
            "  230 |     0.1667 |   1.666667\n",
            "  240 |     0.1667 |   1.666667\n",
            "  250 |     0.1667 |   1.666667\n",
            "  260 |     0.1667 |   1.666667\n",
            "  270 |     0.1667 |   1.666667\n",
            "  280 |     0.1667 |   1.666667\n",
            "  290 |     0.1667 |   1.666667\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}